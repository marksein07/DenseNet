{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import shutil\n",
    "\n",
    "print(torch.__version__)\n",
    "# torch.manual_seed(1)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "DOWNLOAD_MNIST = True\n",
    "preceed=False\n",
    "log='log1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transformation = [\n",
    "    #transforms.RandomGrayscale(p=1.0),\n",
    "    transforms.RandomCrop(32),\n",
    "    #transforms.ColorJitter(0.1,0.1,0.1,0.1),\n",
    "    transforms.RandomHorizontalFlip(p=1.0),\n",
    "    #transforms.RandomVerticalFlip(p=1.0), \n",
    "    ]\n",
    "random_transformation = [transforms.RandomChoice(transformation)]\n",
    "augmentation_transform = transforms.Compose(\n",
    "    [transforms.RandomApply(random_transformation, p=1),\n",
    "     transforms.ToTensor(), ])\n",
    "     #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ])\n",
    "normal_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), ])\n",
    "     #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./mnist', train=True,\n",
    "                                        download=True, transform=normal_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "augmentation_trainset = torchvision.datasets.CIFAR10(root='./mnist', train=True,\n",
    "                                        download=True, transform=augmentation_transform)\n",
    "augmentation_trainloader = torch.utils.data.DataLoader(augmentation_trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./mnist', train=False,\n",
    "                                       download=True, transform=normal_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=True, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img# * 0.5 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(BATCH_SIZE)))\n",
    "#print(images[0])\n",
    "#img = torch.ones((3,32,32))\n",
    "#imshow(torchvision.utils.make_grid(img))\n",
    "print(images[0].size())\n",
    "print(torch.mean(images,dim=[2,3],keepdim=True).size())\n",
    "print(torch.std (images,dim=[2,3],keepdim=True).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        B = True, \n",
    "        C = False, \n",
    "        L = 12,\n",
    "        k = 32,\n",
    "        theta = 0.5,\n",
    "        first_layer = 16\n",
    "    ):\n",
    "        super(CNN, self).__init__()\n",
    "        self.L = L\n",
    "        self.k = k\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.theta = theta\n",
    "        self.first_layer = first_layer\n",
    "        self.in_channels = [self.first_layer] + [k] + [k]\n",
    "        self.conv = nn.ModuleList( [\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(3, self.in_channels[0], 7, 2, 3,bias=False), ),\n",
    "            nn.Sequential(                \n",
    "                nn.BatchNorm2d(int(self.in_channels[1] * (theta if C else 1))),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(self.in_channels[1], int(self.in_channels[2] * (theta if C else 1)), 1, 1, 1,bias=False), ),\n",
    "            nn.Sequential(\n",
    "                nn.BatchNorm2d(int(self.in_channels[2] * (theta if C else 1))),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(k, int(self.in_channels[2] * (theta if C else 1)), 1, 1, 1,bias=False), \n",
    "            ),\n",
    "                ] )\n",
    "        self.denseblock = nn.ModuleList(\n",
    "            [self._build_denseblock( \n",
    "                int(self.in_channels[i] * (theta if C else 1)) if i > 0 else self.in_channels[0] ) \n",
    "             for i in range(len(self.in_channels))]\n",
    "        )\n",
    "        #self.DenseBlock = nn.ModuleList(denseblock)\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.AdaptiveAvgPool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.out = nn.Sequential( \n",
    "            nn.Linear(k, 1024,bias=False),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024,10,bias=False), )\n",
    "        #self.dropout = nn.Dropout(0.5)\n",
    "    def _build_denseblock(self, n_in_channel) :\n",
    "        modules = []\n",
    "        layers = [\n",
    "            nn.BatchNorm2d(n_in_channel),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(n_in_channel, self.k, 1, 1, 0,bias=False)] if self.B else []\n",
    "        layers.extend([\n",
    "            nn.BatchNorm2d(self.k),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(self.k, self.k, 3, 1, 1,bias=False), ])\n",
    "        modules.append(nn.Sequential(*layers))\n",
    "        for l in range(1,self.L) :\n",
    "            layers = [nn.Conv2d(n_in_channel+self.k*(l), n_in_channel+self.k*(l), 1, 1, 0, bias=False)] if self.B else []\n",
    "            layers.extend([\n",
    "                nn.BatchNorm2d(n_in_channel+self.k*(l)),\n",
    "                nn.ReLU(), \n",
    "                nn.Conv2d(n_in_channel+self.k*(l), self.k, 3, 1, 1,bias=False),\n",
    "                nn.Dropout(0.5), ])\n",
    "            modules.append(nn.Sequential(*layers))\n",
    "        return nn.ModuleList(modules)\n",
    "    def _denseblock_feedforward(self, xs, modulelist):\n",
    "        cat = xs\n",
    "        for layer in modulelist :\n",
    "            output = layer(cat)\n",
    "            cat = torch.cat( ( cat, output ), 1 )\n",
    "        return output\n",
    "    def forward(self, xs):\n",
    "        output = xs\n",
    "        for i in range(3) :\n",
    "            #print(output.size())\n",
    "            output = self.conv[i](output)\n",
    "            #print(output.size(), end='\\n\\n')\n",
    "            pool = self.pool2 if i else self.pool1 \n",
    "            output = pool(output)\n",
    "            output = self._denseblock_feedforward(output, self.denseblock[i])\n",
    "        AdaptiveAvgPool = self.AdaptiveAvgPool(output).squeeze()\n",
    "        #print(AdaptiveAvgPool.size())\n",
    "        return self.out(AdaptiveAvgPool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as cp\n",
    "from collections import OrderedDict\n",
    "#from .utils import load_state_dict_from_url\n",
    "from torch import Tensor\n",
    "from torch.jit.annotations import List\n",
    "\n",
    "\n",
    "__all__ = ['DenseNet', 'densenet121', 'densenet169', 'densenet201', 'densenet161']\n",
    "\n",
    "model_urls = {\n",
    "    'densenet121': 'https://download.pytorch.org/models/densenet121-a639ec97.pth',\n",
    "    'densenet169': 'https://download.pytorch.org/models/densenet169-b2777c0a.pth',\n",
    "    'densenet201': 'https://download.pytorch.org/models/densenet201-c1103571.pth',\n",
    "    'densenet161': 'https://download.pytorch.org/models/densenet161-8d451a50.pth',\n",
    "}\n",
    "\n",
    "\n",
    "class _DenseLayer(nn.Module):\n",
    "    def __init__(self, num_input_features, growth_rate, \n",
    "                 bn_size, drop_rate, bias , memory_efficient=True):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
    "                                           growth_rate, kernel_size=1, stride=1, bias=bias,)),\n",
    "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                                           kernel_size=3, stride=1, padding=1, bias=bias,)),\n",
    "        self.drop_rate = float(drop_rate)\n",
    "        self.memory_efficient = memory_efficient\n",
    "\n",
    "    def bn_function(self, inputs):\n",
    "        # type: (List[Tensor]) -> Tensor\n",
    "        concated_features = torch.cat(inputs, 1)\n",
    "        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))  # noqa: T484\n",
    "        return bottleneck_output\n",
    "\n",
    "    # todo: rewrite when torchscript supports any\n",
    "    def any_requires_grad(self, input):\n",
    "        # type: (List[Tensor]) -> bool\n",
    "        for tensor in input:\n",
    "            if tensor.requires_grad:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    @torch.jit.unused  # noqa: T484\n",
    "    def call_checkpoint_bottleneck(self, input):\n",
    "        # type: (List[Tensor]) -> Tensor\n",
    "        def closure(*inputs):\n",
    "            return self.bn_function(*inputs)\n",
    "\n",
    "        return cp.checkpoint(closure, input)\n",
    "\n",
    "    @torch.jit._overload_method  # noqa: F811\n",
    "    def forward(self, input):\n",
    "        # type: (List[Tensor]) -> (Tensor)\n",
    "        pass\n",
    "\n",
    "    @torch.jit._overload_method  # noqa: F811\n",
    "    def forward(self, input):\n",
    "        # type: (Tensor) -> (Tensor)\n",
    "        pass\n",
    "\n",
    "    # torchscript does not yet support *args, so we overload method\n",
    "    # allowing it to take either a List[Tensor] or single Tensor\n",
    "    def forward(self, input):  # noqa: F811\n",
    "        if isinstance(input, Tensor):\n",
    "            prev_features = [input]\n",
    "        else:\n",
    "            prev_features = input\n",
    "\n",
    "        if self.memory_efficient and self.any_requires_grad(prev_features):\n",
    "            if torch.jit.is_scripting():\n",
    "                raise Exception(\"Memory Efficient not supported in JIT\")\n",
    "\n",
    "            bottleneck_output = self.call_checkpoint_bottleneck(prev_features)\n",
    "        else:\n",
    "            bottleneck_output = self.bn_function(prev_features)\n",
    "\n",
    "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate,\n",
    "                                     training=self.training, inplace=True)\n",
    "        return new_features\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.ModuleDict):\n",
    "    _version = 2\n",
    "\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, \n",
    "                 bias, memory_efficient=True,):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            \n",
    "            layer = _DenseLayer(\n",
    "                num_input_features + i * growth_rate,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient,\n",
    "                bias=bias,\n",
    "            )\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "    def forward(self, init_features):\n",
    "        features = [init_features]\n",
    "        #print(init_features.size())\n",
    "        for name, layer in self.items():\n",
    "            new_features = layer(features)\n",
    "            features.append(new_features)\n",
    "        return torch.cat(features, 1)\n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features,bias):\n",
    "        super(_Transition, self).__init__()\n",
    "        #self.add_module('ReflectionPad', nn.ReflectionPad2d(2))\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=bias,))\n",
    "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    r\"\"\"Densenet-BC model class, based on\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "    Args:\n",
    "        growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
    "        block_config (list of 4 ints) - how many layers in each pooling block\n",
    "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
    "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
    "          (i.e. bn_size * k features in the bottleneck layer)\n",
    "        drop_rate (float) - dropout rate after each dense layer\n",
    "        num_classes (int) - number of classification classes\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
    "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, growth_rate=(12,12,12), block_config=(40,40,40),\n",
    "                 num_init_features=16, bn_size=4, drop_rate=0.2, num_classes=10,\n",
    "                 memory_efficient=False, bias=False):\n",
    "\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        # First convolution\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=1,\n",
    "                                padding=3,\n",
    "                                bias=bias, )),\n",
    "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
    "            ('relu0', nn.ReLU(inplace=True)),\n",
    "            #('pool0', nn.AvgPool2d(kernel_size=3, stride=2, padding=1)),\n",
    "        ]))\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        for i, (k, num_layers) in enumerate(zip(growth_rate, block_config)):\n",
    "            print(k,num_layers)\n",
    "            block = _DenseBlock(\n",
    "                num_layers=num_layers,\n",
    "                num_input_features=num_features,\n",
    "                bn_size=bn_size,\n",
    "                growth_rate=k,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient,\n",
    "                bias=bias\n",
    "            )\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * k\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features,\n",
    "                                    num_output_features=num_features // 2, bias=bias)\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "\n",
    "        # Linear layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_features, num_classes, bias=True,) )\n",
    "\n",
    "        # Official init from torch repo.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        #print(out.size())\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 40\n",
      "12 40\n",
      "12 40\n"
     ]
    }
   ],
   "source": [
    "LR = 1e-1\n",
    "\n",
    "cnn = DenseNet()\n",
    "# !!!!!!!! Change in here !!!!!!!!! #\n",
    "cnn.cuda()      # Moves all model parameters and buffers to the GPU.\n",
    "#cnn = torch.nn.DataParallel(cnn,device_ids=[0,1]).cuda()\n",
    "if preceed :\n",
    "    cnn.load_state_dict(torch.load(\"DenseNetL=100,k=12\"))\n",
    "\n",
    "SGD     = torch.optim.SGD\n",
    "Adagrad = torch.optim.Adagrad\n",
    "Adam    = torch.optim.Adam\n",
    "\n",
    "opt = SGD\n",
    "\n",
    "optimizer = opt(cnn.parameters(), lr=LR, weight_decay=1e-4, momentum=0.9)\n",
    "#optimizer = opt(cnn.parameters(), lr=LR, weight_decay=1e-3)\n",
    "#semantic_optimizer = torch.optim.Adagrad(cnn.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "shutil.rmtree(log)\n",
    "writer = SummaryWriter(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-2\n",
    "optimizer = opt(cnn.parameters(), lr=LR, momentum = 0.9, weight_decay=1e-4)\n",
    "#cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [05:42,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   782] Training loss: 1.6234    Training Accuracy : 0.4204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "157it [00:20,  7.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   157]  Testing loss: 1.4673    Testing  Accuracy : 0.4889\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "782it [05:42,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,   782] Training loss: 1.1295    Training Accuracy : 0.6013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "157it [00:20,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,   157]  Testing loss: 1.1254    Testing  Accuracy : 0.6109\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "782it [05:41,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3,   782] Training loss: 0.8610    Training Accuracy : 0.7009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "157it [00:20,  7.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3,   157]  Testing loss: 0.9184    Testing  Accuracy : 0.7024\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "782it [05:41,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4,   782] Training loss: 0.6730    Training Accuracy : 0.7671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "157it [00:20,  7.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4,   157]  Testing loss: 0.9644    Testing  Accuracy : 0.7123\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "782it [05:42,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5,   782] Training loss: 0.5652    Training Accuracy : 0.8055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "157it [00:20,  7.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5,   157]  Testing loss: 0.6370    Testing  Accuracy : 0.7892\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "214it [01:33,  2.28it/s]"
     ]
    }
   ],
   "source": [
    "epoch_duration = 1\n",
    "max_epoch = 300\n",
    "\n",
    "for epoch in range(epoch, max_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    training_accuracy_list = []\n",
    "    #semantic_loss_list = []\n",
    "    cnn.train()\n",
    "    if epoch > 1 and (epoch % 50 == 0 or epoch % 75 == 0) :\n",
    "        LR/=10;\n",
    "        optimizer = opt(cnn.parameters(), lr=LR, momentum=0.9, weight_decay=1e-4)\n",
    "        print(\"Optimizer updated.\")\n",
    "        \n",
    "    for i, data in tqdm(enumerate(trainloader, 1)):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.cuda()\n",
    "        inputs = ( inputs - inputs.mean(dim=[2,3], keepdim=True) ) \\\n",
    "                    / inputs.std(dim=[2,3], keepdim=True) * 1e-2\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = cnn(inputs)\n",
    "        ys = outputs.argmax(dim = 1).detach().cpu()\n",
    "        result = torch.eq(ys,labels)\n",
    "        training_accuracy_list.append(result.type(torch.FloatTensor).mean().item())\n",
    "        crossEntropyLoss = criterion(outputs, labels.cuda())\n",
    "        crossEntropyLoss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += crossEntropyLoss.detach().cpu().item()\n",
    "        \n",
    "#     for i, data in tqdm(enumerate(augmentation_trainloader, 1)):\n",
    "#         # get the inputs; data is a list of [inputs, labels]\n",
    "#         inputs, labels = data\n",
    "#         inputs = inputs.cuda()\n",
    "#         inputs = ( inputs - inputs.mean(dim=[2,3], keepdim=True) ) \\\n",
    "#                     / inputs.std(dim=[2,3], keepdim=True) * 1e-2\n",
    "        \n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # forward + backward + optimize\n",
    "#         outputs = cnn(inputs)\n",
    "#         ys = outputs.argmax(dim = 1).detach().cpu()\n",
    "#         result = torch.eq(ys,labels)\n",
    "#         training_accuracy_list.append(result.type(torch.FloatTensor).mean().item())\n",
    "#         crossEntropyLoss = criterion(outputs, labels.cuda())\n",
    "#         crossEntropyLoss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         # print statistics\n",
    "#         running_loss += crossEntropyLoss.detach().cpu().item()\n",
    "        \n",
    "    if i % epoch_duration == (epoch_duration - 1):    # print every 2000 mini-batches\n",
    "        #writer.add_scalar('Training/Loss', running_loss / i, epoch + 1)\n",
    "        print('[%d, %5d] Training loss: %.4f    Training Accuracy : %.4f'% # Semantic loss : %.4f' %\n",
    "              (epoch + 1, \n",
    "               i, \n",
    "               running_loss / i, \n",
    "               np.mean(training_accuracy_list), )\n",
    "               #np.mean(semantic_loss_list) )\n",
    "             ) \n",
    "        #running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        testing_loss = 0.0\n",
    "        testing_accuracy_list = []\n",
    "        cnn.eval()\n",
    "        for j, data in tqdm( enumerate(testloader, 1) ):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.cuda()\n",
    "            inputs = ( inputs - inputs.mean(dim=[2,3], keepdim=True) ) \\\n",
    "                        / inputs.std(dim=[2,3], keepdim=True) * 1e-2\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            outputs = cnn(inputs)\n",
    "            ys = outputs.argmax(dim = 1).detach().cpu()\n",
    "            result = torch.eq(ys, labels)\n",
    "            testing_accuracy_list.append(result.type(torch.FloatTensor).mean().item())\n",
    "            loss = criterion(outputs, labels.cuda()).detach().cpu().item()\n",
    "            #loss.backward()\n",
    "            #optimizer.step()\n",
    "\n",
    "            testing_loss += loss\n",
    "\n",
    "    print('[%d, %5d]  Testing loss: %.4f    Testing  Accuracy : %.4f\\n' % \n",
    "          ( epoch + 1, \n",
    "           j, \n",
    "           testing_loss / j, \n",
    "           np.mean(testing_accuracy_list) ))\n",
    "    writer.add_scalars('cifar10/Accuracy', \n",
    "                       {'Training' : np.mean(training_accuracy_list),\n",
    "                        'Testing'  : np.mean(testing_accuracy_list), \n",
    "                       }, epoch + 1)\n",
    "    writer.add_scalars('cifar10/loss', \n",
    "                       {'Training' : running_loss / i, \n",
    "                        'Testing'  : testing_loss / j, \n",
    "                        #'Semantic' : np.mean(semantic_loss_list),    \n",
    "                       }, epoch + 1)\n",
    "        \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn = cnn.cpu()\n",
    "torch.save(cnn.state_dict(), \"DenseNetL=100,k=12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.load_state_dict(torch.load(\"DenseNetL=100,k=12\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as cp\n",
    "from collections import OrderedDict\n",
    "#from .utils import load_state_dict_from_url\n",
    "from torch import Tensor\n",
    "from torch.jit.annotations import List\n",
    "\n",
    "\n",
    "__all__ = ['DenseNet', 'densenet121', 'densenet169', 'densenet201', 'densenet161']\n",
    "\n",
    "model_urls = {\n",
    "    'densenet121': 'https://download.pytorch.org/models/densenet121-a639ec97.pth',\n",
    "    'densenet169': 'https://download.pytorch.org/models/densenet169-b2777c0a.pth',\n",
    "    'densenet201': 'https://download.pytorch.org/models/densenet201-c1103571.pth',\n",
    "    'densenet161': 'https://download.pytorch.org/models/densenet161-8d451a50.pth',\n",
    "}\n",
    "\n",
    "\n",
    "class _DenseLayer(nn.Module):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=True):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
    "                                           growth_rate, kernel_size=1, stride=1,\n",
    "                                           bias=False)),\n",
    "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                                           kernel_size=3, stride=1, padding=1,\n",
    "                                           bias=False)),\n",
    "        self.drop_rate = float(drop_rate)\n",
    "        self.memory_efficient = memory_efficient\n",
    "\n",
    "    def bn_function(self, inputs):\n",
    "        # type: (List[Tensor]) -> Tensor\n",
    "        concated_features = torch.cat(inputs, 1)\n",
    "        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))  # noqa: T484\n",
    "        return bottleneck_output\n",
    "\n",
    "    # todo: rewrite when torchscript supports any\n",
    "    def any_requires_grad(self, input):\n",
    "        # type: (List[Tensor]) -> bool\n",
    "        for tensor in input:\n",
    "            if tensor.requires_grad:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    @torch.jit.unused  # noqa: T484\n",
    "    def call_checkpoint_bottleneck(self, input):\n",
    "        # type: (List[Tensor]) -> Tensor\n",
    "        def closure(*inputs):\n",
    "            return self.bn_function(*inputs)\n",
    "\n",
    "        return cp.checkpoint(closure, input)\n",
    "\n",
    "    @torch.jit._overload_method  # noqa: F811\n",
    "    def forward(self, input):\n",
    "        # type: (List[Tensor]) -> (Tensor)\n",
    "        pass\n",
    "\n",
    "    @torch.jit._overload_method  # noqa: F811\n",
    "    def forward(self, input):\n",
    "        # type: (Tensor) -> (Tensor)\n",
    "        pass\n",
    "\n",
    "    # torchscript does not yet support *args, so we overload method\n",
    "    # allowing it to take either a List[Tensor] or single Tensor\n",
    "    def forward(self, input):  # noqa: F811\n",
    "        if isinstance(input, Tensor):\n",
    "            prev_features = [input]\n",
    "        else:\n",
    "            prev_features = input\n",
    "        #print(self.any_requires_grad(prev_features))\n",
    "        if self.memory_efficient and self.any_requires_grad(prev_features):\n",
    "            if torch.jit.is_scripting():\n",
    "                raise Exception(\"Memory Efficient not supported in JIT\")\n",
    "\n",
    "            bottleneck_output = self.call_checkpoint_bottleneck(prev_features)\n",
    "        else:\n",
    "            bottleneck_output = self.bn_function(prev_features)\n",
    "\n",
    "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate,\n",
    "                                     training=self.training)\n",
    "        return new_features\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.ModuleDict):\n",
    "    _version = 2\n",
    "\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=True):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(\n",
    "                num_input_features + i * growth_rate,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient,\n",
    "            )\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "    def forward(self, init_features):\n",
    "        features = [init_features]\n",
    "        for name, layer in self.items():\n",
    "            new_features = layer(features)\n",
    "            features.append(new_features)\n",
    "        return torch.cat(features, 1)\n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    r\"\"\"Densenet-BC model class, based on\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "    Args:\n",
    "        growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
    "        block_config (list of 4 ints) - how many layers in each pooling block\n",
    "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
    "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
    "          (i.e. bn_size * k features in the bottleneck layer)\n",
    "        drop_rate (float) - dropout rate after each dense layer\n",
    "        num_classes (int) - number of classification classes\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
    "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, growth_rate=12, block_config=(40, 40, 40),\n",
    "                 num_init_features=16, bn_size=4, drop_rate=0.2, num_classes=10, memory_efficient=True):\n",
    "\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        # First convolution\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=1,\n",
    "                                padding=3, bias=False)),\n",
    "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
    "            ('relu0', nn.ReLU(inplace=True)),\n",
    "            #('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
    "        ]))\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(\n",
    "                num_layers=num_layers,\n",
    "                num_input_features=num_features,\n",
    "                bn_size=bn_size,\n",
    "                growth_rate=growth_rate,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient\n",
    "            )\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features,\n",
    "                                    num_output_features=num_features // 2)\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "\n",
    "        # Linear layer\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "        # Official init from torch repo.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
